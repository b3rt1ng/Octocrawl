import asyncio
import json
import sys
import time
import bs4

from urllib.parse import urlparse, urlunparse, urljoin
from http_request import http_request
from parser import html_parser, json_parser, dir_listing_parser
from tree_maker import TreeMaker
from ui import print_status_line, gradient_text, whole_line
from fingerprint import fingerprint_technologies

class crawler:
    def __init__(self, start_url, max_workers=50, timeout=5, cookies=None, parser="lxml", random_agent=False):
        self.start_url = start_url
        self.base_domain = urlparse(start_url).netloc
        self.timeout = timeout
        self.cookies = cookies if cookies is not None else {}
        self.parser_engine = parser
        
        self.queue = asyncio.Queue()
        self.visited_urls = {self._normalize_url(start_url)}
        self.gathered_urls = {}
        self.sitemap = {}
        self.checked_for_listing = set()
        self.listing_discoveries = set()

        self.max_workers = max_workers
        self.print_lock = asyncio.Lock()
        self.sitemap_lock = asyncio.Lock()
        self.queue_lock = asyncio.Lock()
        self.worker_tasks = []

        self.technologies = {}
        self.tech_lock = asyncio.Lock()
        self.random_agent = random_agent

    @staticmethod
    def _normalize_url(url):
        return urlunparse(urlparse(url)._replace(query='', fragment=''))

    def _add_to_sitemap(self, url, url_data):
        parsed_url = urlparse(url)
        path_segments = parsed_url.path.strip('/').split('/') if parsed_url.path.strip('/') else []
        current_level = self.sitemap
        if not path_segments or (len(path_segments) == 1 and path_segments[0] == ''):
            self.sitemap.setdefault('/', {}).update({'_data': url_data})
            return
        for segment in path_segments[:-1]:
            if not segment: continue
            current_level = current_level.setdefault(segment, {})
        last_segment = path_segments[-1]
        if last_segment:
             current_level.setdefault(last_segment, {}).update({'_data': url_data})

    async def worker(self, keywords=None, is_listing_phase=False):
        while True:
            url_to_process = None
            try:
                url_to_process = await self.queue.get()

                request = await http_request(url_to_process, timeout=self.timeout, cookies=self.cookies, random_agent=self.random_agent)
                
                async with self.print_lock:
                    print_status_line(f"Checked: {url_to_process} [{request['response_code']}]")

                if request['done']:
                    found_tech = fingerprint_technologies(request['headers'], request['content'])
                    if found_tech:
                        async with self.tech_lock:
                            self.technologies.update(found_tech)

                canonical_url = self._normalize_url(url_to_process)
                
                async with self.sitemap_lock:
                    if canonical_url in self.gathered_urls:
                        self.queue.task_done()
                        continue

                url_data = { 
                    'code': request["response_code"], 
                    'content_type': request["content_type"], 
                    'url': canonical_url, 
                    'keywords': {},
                    'from_listing': is_listing_phase
                }

                if request["done"] and request["response_code"] == 200 and request["content"]:
                    parser = None
                    content, ctype = request["content"], request["content_type"]

                    if 'html' in ctype:
                        soup = bs4.BeautifulSoup(content, self.parser_engine)
                        is_listing = soup.title and "Index of /" in soup.title.string
                        parser = dir_listing_parser(content, canonical_url, soup=soup, parser=self.parser_engine) if is_listing else html_parser(content, canonical_url, soup=soup, parser=self.parser_engine)
                    elif 'json' in ctype:
                        parser = json_parser(content, canonical_url)
                    
                    if parser:
                        for link in parser.internal_links:
                            normalized_link = self._normalize_url(link)
                            async with self.queue_lock:
                                if normalized_link not in self.visited_urls:
                                    self.visited_urls.add(normalized_link)
                                    await self.queue.put(normalized_link)
                                    if is_listing_phase:
                                        self.listing_discoveries.add(normalized_link)
                        if keywords:
                            url_data['keywords'] = parser.find_keywords(keywords)
                
                async with self.sitemap_lock:
                    self.gathered_urls[canonical_url] = url_data
                    self._add_to_sitemap(canonical_url, url_data)

            except asyncio.CancelledError:
                break
            except Exception as e:
                async with self.print_lock:
                    print(f"\n[!] Error processing {url_to_process}: {type(e).__name__}. Skipping.")
            finally:
                if url_to_process:
                    self.queue.task_done()

    async def _crawl_loop(self, keywords=None, is_listing_phase=False):
        self.worker_tasks = [asyncio.create_task(self.worker(keywords, is_listing_phase)) for _ in range(self.max_workers)]
        await self.queue.join()

    def _get_all_directory_urls(self, sitemap_node=None, current_path_url=None):
        if sitemap_node is None: sitemap_node = self.sitemap
        if current_path_url is None: current_path_url = self.start_url
        dir_urls = set()
        for key, value in sitemap_node.items():
            children = {k: v for k, v in value.items() if k != '_data'}
            if children:
                base_for_join = current_path_url if current_path_url.endswith('/') else current_path_url + '/'
                dir_url = urljoin(base_for_join, key + '/')
                dir_urls.add(dir_url)
                dir_urls.update(self._get_all_directory_urls(children, dir_url))
        return dir_urls
    
    def _filter_sitemap_for_listing(self, sitemap_node=None):
        if sitemap_node is None:
            sitemap_node = self.sitemap
        
        filtered = {}
        for key, value in sitemap_node.items():
            if key == '_data':
                url = value.get('url', '')
                if url in self.listing_discoveries or value.get('from_listing', False):
                    filtered[key] = value
            else:
                children = {k: v for k, v in value.items() if k != '_data'}
                filtered_children = self._filter_sitemap_for_listing(children)
                
                if '_data' in value:
                    url = value['_data'].get('url', '')
                    if url in self.listing_discoveries or value['_data'].get('from_listing', False):
                        filtered[key] = {'_data': value['_data']}
                        if filtered_children:
                            filtered[key].update(filtered_children)
                    elif filtered_children:
                        filtered[key] = filtered_children
                elif filtered_children:
                    filtered[key] = filtered_children
        
        return filtered
    
    async def shutdown(self):
        for task in self.worker_tasks:
            task.cancel()
        await asyncio.gather(*self.worker_tasks, return_exceptions=True)

    async def crawl(self, show_url_in_tree=False, noshow_extensions=None, keywords=None, explore_directories=False, show_only_listing=False, output_file=None):
        start_time = time.time()
        self.queue.put_nowait(self.start_url)
        
        try:
            await self._crawl_loop(keywords, is_listing_phase=False)

            if explore_directories:
                print(f"\r{whole_line()}")
                async with self.print_lock: print(gradient_text("üêô Going for directory exploration"))
                while True:
                    all_dirs = self._get_all_directory_urls()
                    new_dirs_to_check = all_dirs - self.checked_for_listing
                    if not new_dirs_to_check: break
                    
                    async with self.queue_lock:
                        for d in new_dirs_to_check:
                            normalized_dir = self._normalize_url(d)
                            if normalized_dir not in self.visited_urls:
                                self.visited_urls.add(normalized_dir)
                                await self.queue.put(d)
                                self.listing_discoveries.add(normalized_dir)
                    
                    self.checked_for_listing.update(new_dirs_to_check)
                    await self._crawl_loop(keywords, is_listing_phase=True)
                    
        except asyncio.CancelledError:
            async with self.print_lock: print(gradient_text("\nüêô Crawl cancelled by user. Exiting."))
        finally:
            await self.shutdown()
        
        end_time = time.time()
        
        async with self.print_lock: print(f"\r{whole_line()}")
        print(gradient_text("üêô Crawl finished. Generating sitemap tree..."))
        
        sitemap_to_display = self.sitemap
        if show_only_listing and explore_directories:
            sitemap_to_display = self._filter_sitemap_for_listing()
            if not sitemap_to_display:
                print(gradient_text("‚ö†Ô∏è  No URLs found via directory listing."))
        
        tree_maker = TreeMaker(base_url=self.start_url, noshow=noshow_extensions)
        tree_maker.print_tree(sitemap_to_display, show_url=show_url_in_tree)

        total_urls = len(self.gathered_urls)
        listing_urls = len(self.listing_discoveries) if explore_directories else 0
        
        if show_only_listing and explore_directories:
            summary_line = f"\nShowing {listing_urls} URLs discovered via directory listing (out of {total_urls} total URLs) in {round(end_time-start_time, 3)} seconds"
        else:
            summary_line = f"\nGathered {total_urls} unique URLs"
            if explore_directories:
                summary_line += f" ({listing_urls} via directory listing)"
            summary_line += f" in {round(end_time-start_time, 3)} seconds"
        
        print(summary_line)

        if output_file:
            print(f"Saving report to {output_file}...")
            try:
                if output_file.lower().endswith('.json'):
                    with open(output_file, 'w', encoding='utf-8') as f:
                        json.dump(sitemap_to_display, f, indent=4)
                else:
                    with open(output_file, 'w', encoding='utf-8') as f:
                        tree_maker.print_tree(sitemap_to_display, show_url=show_url_in_tree, output_stream=f)
                        f.write(summary_line + "\n")
                print(f"Report saved successfully.")
            except Exception as e:
                print(f"\nError saving report to {output_file}: {e}", file=sys.stderr)
